{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b580359-84a6-474a-bcdf-1f098da8e87f",
   "metadata": {},
   "source": [
    "# __HoWDe__ \n",
    "### _a Home and Work location Detection algorithm for GPS data analytics_\n",
    "\n",
    "This notebook is intended to work as a brief tutorial on how to user \"HoWDe\". It leverages functions contained in the \"HoWDe_utils.py\" file (source code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dd5a79-6d0a-4463-844d-f995694edcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "# # NB: Home and Work location labelling\n",
    "# 1. Load pre-computed stop locations (in our case computed using infostop)\n",
    "# 2. Assigning labels to stop locations: Home (\"H\"), Work (\"W\"), and Other (\"O\")\n",
    "from howde import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90caa6b-93dd-4ce3-b10e-f9e1974bd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_PATH = None\n",
    "HW_PATH = '/Users/lorentz/JupyterDir/20-04_COVID_all/World_Bank/02-home_work_detection/data/stop_location_results/veraset_location_clustered/'\n",
    "\n",
    "\n",
    "try: assert type(HW_PATH) is str\n",
    "except: print(\"Path to data is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c43d5-5143-4a76-9515-f674297e6962",
   "metadata": {},
   "source": [
    "#### 1. Use HoWDe providing pre-loaded data and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8db261-bb1f-4a27-b456-e9ff9daffa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load DATA and PASS it to HoWDe\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up Spark\n",
    "driver_memory=250\n",
    "packages = \"data/work/shared/tools/spark-avro_2.12-3.0.0.jar\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--jars {0} pyspark-shell \".format(packages)\n",
    "spark = (SparkSession\n",
    "            .builder.master(\"local[50]\")\n",
    "            .config(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "            .config(\"spark.driver.memory\", f\"{driver_memory}g\")\n",
    "            .config(\"spark.executor.memory\", \"250g\")\n",
    "            .getOrCreate()\n",
    ")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Load DATA\n",
    "input_data = spark.read.format(\"parquet\").load(HW_PATH, pathGlobFilter=\"*.parquet\")\n",
    "\n",
    "res1 = HoWDe_labelling(input_data = input_data, spark=spark, HW_PATH='./',\n",
    "                    SAVE_PATH=None, SAVE_NAME='', save_multiple=False,\n",
    "                    edit_config_default=None, \n",
    "                    range_window=42, bnd_none_day=6,\n",
    "                    bnd_none_home=[0.4,0.6], bnd_none_work=0.8,\n",
    "                    range_freq_home=0.2, range_freq_work_h=0.2,\n",
    "                    range_freq_work_d=0.2,\n",
    "                    driver_memory = 250\n",
    "                   )\n",
    "\n",
    "res1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3f0e8-f6bd-49e0-9000-9f3dc122b6e6",
   "metadata": {},
   "source": [
    "#### 2. Use HoWDe in a self contained way (providing path to data and location to save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f249eb6d-de2f-4453-86d2-285060f191cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/23 17:11:36 WARN Utils: Your hostname, LooP.local resolves to a loopback address: 127.0.0.1; using 192.168.129.24 instead (on interface en0)\n",
      "25/01/23 17:11:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/01/23 17:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/23 17:11:37 WARN DependencyUtils: Local jar /Users/lorentz/JupyterDir/GIT/projects/home_work_detection/HoWDe/data/work/shared/tools/spark-avro_2.12-3.0.0.jar does not exist, skipping.\n",
      "25/01/23 17:11:37 INFO SparkContext: Running Spark version 3.5.4\n",
      "25/01/23 17:11:37 INFO SparkContext: OS info Mac OS X, 15.1.1, aarch64\n",
      "25/01/23 17:11:37 INFO SparkContext: Java version 19.0.2\n",
      "25/01/23 17:11:37 INFO ResourceUtils: ==============================================================\n",
      "25/01/23 17:11:37 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/23 17:11:37 INFO ResourceUtils: ==============================================================\n",
      "25/01/23 17:11:37 INFO SparkContext: Submitted application: pyspark-shell\n",
      "25/01/23 17:11:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 256000, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/23 17:11:37 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/01/23 17:11:37 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/23 17:11:37 INFO SecurityManager: Changing view acls to: lorentz\n",
      "25/01/23 17:11:37 INFO SecurityManager: Changing modify acls to: lorentz\n",
      "25/01/23 17:11:37 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/23 17:11:37 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/23 17:11:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: lorentz; groups with view permissions: EMPTY; users with modify permissions: lorentz; groups with modify permissions: EMPTY\n",
      "25/01/23 17:11:37 INFO Utils: Successfully started service 'sparkDriver' on port 64090.\n",
      "25/01/23 17:11:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/23 17:11:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/23 17:11:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/23 17:11:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/23 17:11:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/23 17:11:37 INFO DiskBlockManager: Created local directory at /private/var/folders/37/srrcmpqx22j0qx7gjc4j7x3r0000gq/T/blockmgr-1d37edec-01bc-4eb2-8f32-14d65e58bf1e\n",
      "25/01/23 17:11:37 INFO MemoryStore: MemoryStore started with capacity 149.8 GiB\n",
      "25/01/23 17:11:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/23 17:11:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/01/23 17:11:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/23 17:11:37 ERROR SparkContext: Failed to add file:/Users/lorentz/JupyterDir/GIT/projects/home_work_detection/HoWDe/data/work/shared/tools/spark-avro_2.12-3.0.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/lorentz/JupyterDir/GIT/projects/home_work_detection/HoWDe/data/work/shared/tools/spark-avro_2.12-3.0.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "25/01/23 17:11:37 INFO Executor: Starting executor ID driver on host 192.168.129.24\n",
      "25/01/23 17:11:37 INFO Executor: OS info Mac OS X, 15.1.1, aarch64\n",
      "25/01/23 17:11:37 INFO Executor: Java version 19.0.2\n",
      "25/01/23 17:11:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/01/23 17:11:37 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@63db65f0 for default.\n",
      "25/01/23 17:11:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64091.\n",
      "25/01/23 17:11:37 INFO NettyBlockTransferService: Server created on 192.168.129.24:64091\n",
      "25/01/23 17:11:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/23 17:11:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.129.24, 64091, None)\n",
      "25/01/23 17:11:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.129.24:64091 with 149.8 GiB RAM, BlockManagerId(driver, 192.168.129.24, 64091, None)\n",
      "25/01/23 17:11:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.129.24, 64091, None)\n",
      "25/01/23 17:11:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.129.24, 64091, None)\n",
      "25/01/23 17:11:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/01/23 17:11:37 INFO SharedState: Warehouse path is 'file:/Users/lorentz/JupyterDir/GIT/projects/home_work_detection/HoWDe/spark-warehouse'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoWDe Labelling: computing LABs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoWDe Labelling: computations completed!\n",
      "root\n",
      " |-- useruuid: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- loc: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- stop_duration: long (nullable = true)\n",
      " |-- location_type: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Let HoWDe load data \n",
    "res2 = HoWDe_labelling(input_data=None, spark=None, HW_PATH=HW_PATH,\n",
    "                    SAVE_PATH=None, SAVE_NAME='', save_multiple=False,\n",
    "                    edit_config_default=None, \n",
    "                    range_window=42, bnd_none_day=6,\n",
    "                    bnd_none_home=[0.4,0.6], bnd_none_work=0.8,\n",
    "                    range_freq_home=0.2, range_freq_work_h=0.2,\n",
    "                    range_freq_work_d=0.2,\n",
    "                    driver_memory = 250\n",
    "                   )\n",
    "res2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd69994-3d2c-4f61-9147-90df69dc4a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/HoWDe/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=13486) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---+----------+----------+-------------------+----------+----------+-------------+-------------+\n",
      "|            useruuid|country|loc|       lat|       lon|               date|     start|       end|stop_duration|location_type|\n",
      "+--------------------+-------+---+----------+----------+-------------------+----------+----------+-------------+-------------+\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63578| -46.56485|2021-06-06 00:00:00|1623005440|1623006208|          768|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63582| -46.56487|2021-06-07 00:00:00|1623020288|1623023872|         3584|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63579| -46.56487|2021-06-07 00:00:00|1623032448|1623034752|         2304|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63582| -46.56488|2021-06-07 00:00:00|1623062528|1623063424|          896|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63583| -46.56485|2021-06-07 00:00:00|1623068032|1623068416|          384|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63581| -46.56486|2021-06-07 00:00:00|1623074432|1623074688|          256|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63376|-46.564598|2021-06-07 00:00:00|1623075072|1623075712|          640|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63582|-46.564877|2021-06-07 00:00:00|1623076864|1623088768|        11904|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63595| -46.56429|2021-06-07 00:00:00|1623093888|1623094144|          256|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30|-23.635786|-46.564854|2021-06-07 00:00:00|1623102720|1623103199|          479|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30|-23.635813|-46.564873|2021-06-08 00:00:00|1623143296|1623149824|         6528|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30|-23.635841|-46.564907|2021-06-08 00:00:00|1623160832|1623161984|         1152|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30|-23.635359|-46.564365|2021-06-08 00:00:00|1623162112|1623162752|          640|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63581| -46.56487|2021-06-08 00:00:00|1623166592|1623174528|         7936|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30|-23.635794| -46.56486|2021-06-08 00:00:00|1623175296|1623181440|         6144|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 79|-23.616573|  -46.5654|2021-06-08 00:00:00|1623184256|1623189120|         4864|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63579| -46.56485|2021-06-09 00:00:00|1623190144|1623196800|         6656|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63579| -46.56485|2021-06-09 00:00:00|1623196800|1623199360|         2560|            O|\n",
      "|63c29bed022b6c8dd...|     BR| 80|-23.619982|-46.567375|2021-06-09 00:00:00|1623226240|1623228544|         2304|            W|\n",
      "|63c29bed022b6c8dd...|     BR| 30| -23.63578| -46.56485|2021-06-09 00:00:00|1623247616|1623260928|        13312|            O|\n",
      "+--------------------+-------+---+----------+----------+-------------------+----------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e85c4-4bd9-4d72-8e94-51fa232624d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
