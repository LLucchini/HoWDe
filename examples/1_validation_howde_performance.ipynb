{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88cd0a15",
   "metadata": {},
   "source": [
    "# __HoWDe__ \n",
    "### _a Home and Work location Detection algorithm for GPS data analytics_\n",
    "\n",
    "This notebook is intended to work as a brief tutorial on how to validate \"HoWDe\" against ground-truth data.\n",
    "<!-- Ground truth data is available at: (link in paper)  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335988b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ac0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import howde\n",
    "from howde import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/27 13:39:36 WARN Utils: Your hostname, bohr resolves to a loopback address: 127.0.1.1; using 130.225.68.124 instead (on interface eno1)\n",
      "25/05/27 13:39:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/27 13:39:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/27 13:39:36 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/05/27 13:39:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/27 13:39:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up Spark\n",
    "temp_folder = '/data/work/user/sdsc'\n",
    "mem=\"50g\"\n",
    "n_workers = 10\n",
    "spark = SparkSession.builder.config(\"spark.sql.files.ignoreCorruptFiles\",\"true\")\\\n",
    "                                            .config(\"spark.driver.memory\", mem) \\\n",
    "                                            .config(\"spark.driver.maxResultSize\", \"40g\") \\\n",
    "                                            .config(\"spark.executer.memory\", \"40g\") \\\n",
    "                                            .config(\"spark.local.dir\",temp_folder)\\\n",
    "                                            .config(\"spark.sql.session.timeZone\",\"UTC\")\\\n",
    "                                            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "                                            .config(\"spark.driver.maxResultSize\", \"40g\")\\\n",
    "                                            .config(\"spark.kryoserializer.buffer.max\", \"128m\")\\\n",
    "                                            .config(\"spark.storage.memoryFraction\", \"0.5\")\\\n",
    "                                            .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "                                            .master(f\"local[{n_workers}]\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6369a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder with Data\n",
    "PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7afd6d",
   "metadata": {},
   "source": [
    "## Validation against D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+---+------------------+\n",
      "|useruuid|s_yy|s_woy|loc|true_location_type|\n",
      "+--------+----+-----+---+------------------+\n",
      "|       1|1970|   13| 17|                 W|\n",
      "|       2|1970|    5|  0|                 W|\n",
      "|       3|1970|   22|  2|                 W|\n",
      "|       4|1971|    6|  1|                 H|\n",
      "+--------+----+-----+---+------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "4847\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- IMPORT HOME/WORK GROUND TRUTH - WEEK LEVEL ----------------------\n",
    "fname = 'D1_truelabels_uwy'\n",
    "truelabels_wy = spark.read.format(\"parquet\").load(PATH+fname, pathGlobFilter=\"*.parquet\")\n",
    "\n",
    "# - Show sample of week-level true labels\n",
    "truelabels_wy.orderBy([\"useruuid\", \"s_yy\", \"s_woy\", \"loc\"]).show(4)\n",
    "print(truelabels_wy.select('useruuid').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46c80a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+-------+-------+\n",
      "|useruuid|loc|  start|    end|country|\n",
      "+--------+---+-------+-------+-------+\n",
      "|       1|  0| 674400| 676200|   GL0B|\n",
      "|       1|  1| 678000| 697200|   GL0B|\n",
      "|       1|  1| 717600| 732600|   GL0B|\n",
      "|       1|  1| 767400| 819600|   GL0B|\n",
      "|       1|  2| 820200| 850800|   GL0B|\n",
      "|       1|  1| 852000| 904200|   GL0B|\n",
      "|       1|  0| 933000| 934800|   GL0B|\n",
      "|       1|  1| 935400| 987000|   GL0B|\n",
      "|       1|  1|1026000|1069800|   GL0B|\n",
      "|       1|  2|1088400|1111200|   GL0B|\n",
      "|       1|  1|1113600|1159200|   GL0B|\n",
      "|       1|  6|1159800|1163400|   GL0B|\n",
      "|       1| 79|1165200|1168200|   GL0B|\n",
      "|       1|  2|1174800|1197600|   GL0B|\n",
      "|       1|  1|1199400|1251000|   GL0B|\n",
      "|       1|  1|1290600|1337400|   GL0B|\n",
      "|       1|  4|1339800|1341000|   GL0B|\n",
      "|       1| 36|1351200|1353600|   GL0B|\n",
      "|       1| 37|1353600|1362000|   GL0B|\n",
      "|       1|  5|1362600|1370400|   GL0B|\n",
      "+--------+---+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "4847\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- IMPORT STOPS ----------------------\n",
    "fname = 'D1_stops'\n",
    "stops_anonym_sample = spark.read.format(\"parquet\").load(PATH+fname, pathGlobFilter=\"*.parquet\")\n",
    "stops_anonym_sample.orderBy(['useruuid', 'start']).show()\n",
    "print(stops_anonym_sample.select('useruuid').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffb73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoWDe Labelling: computing LABs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> stops pre-processed\n",
      " >>> home/works detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> output formatted as stops: True\n",
      "HoWDe Labelling: computations completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/27 13:39:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- HOME/WORK DETECTION with HoWDe ----------------------\n",
    "stops_c_hw2 = HoWDe_labelling(\n",
    "    input_data = stops_anonym_sample,\n",
    "    spark = spark,\n",
    "    range_window_home = 28,\n",
    "    range_window_work = 42,\n",
    "    dhn = 3,    \n",
    "    dn_H = 0.7, \n",
    "    dn_W = 0.5, \n",
    "    hf_H = 0.7, \n",
    "    hf_W = 0.4, \n",
    "    df_W = 0.6, \n",
    "    stops_output = True,\n",
    "    verbose = True,\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "912bc7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===================================================>  (191 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---+-------------------+------+------+-------------+-------------+------------+------------+-------------------+-------------------+\n",
      "|useruuid|country|loc|date               |start |end   |stop_duration|location_type|detect_H_loc|detect_W_loc|start_ts           |end_ts             |\n",
      "+--------+-------+---+-------------------+------+------+-------------+-------------+------------+------------+-------------------+-------------------+\n",
      "|1       |GL0B   |0  |1970-01-08 00:00:00|674400|676200|1800         |O            |1           |null        |1970-01-08 19:20:00|1970-01-08 19:50:00|\n",
      "|1       |GL0B   |1  |1970-01-08 00:00:00|678000|691199|13199        |H            |1           |null        |1970-01-08 20:20:00|1970-01-08 23:59:59|\n",
      "|1       |GL0B   |1  |1970-01-09 00:00:00|691200|697200|6000         |H            |1           |null        |1970-01-09 00:00:00|1970-01-09 01:40:00|\n",
      "|1       |GL0B   |1  |1970-01-09 00:00:00|717600|732600|15000        |H            |1           |null        |1970-01-09 07:20:00|1970-01-09 11:30:00|\n",
      "|1       |GL0B   |1  |1970-01-09 00:00:00|767400|777599|10199        |H            |1           |null        |1970-01-09 21:10:00|1970-01-09 23:59:59|\n",
      "|1       |GL0B   |1  |1970-01-10 00:00:00|777600|819600|42000        |H            |1           |null        |1970-01-10 00:00:00|1970-01-10 11:40:00|\n",
      "|1       |GL0B   |2  |1970-01-10 00:00:00|820200|850800|30600        |O            |1           |null        |1970-01-10 11:50:00|1970-01-10 20:20:00|\n",
      "|1       |GL0B   |1  |1970-01-10 00:00:00|852000|863999|11999        |H            |1           |null        |1970-01-10 20:40:00|1970-01-10 23:59:59|\n",
      "|1       |GL0B   |1  |1970-01-11 00:00:00|864000|904200|40200        |H            |1           |null        |1970-01-11 00:00:00|1970-01-11 11:10:00|\n",
      "|1       |GL0B   |0  |1970-01-11 00:00:00|933000|934800|1800         |O            |1           |null        |1970-01-11 19:10:00|1970-01-11 19:40:00|\n",
      "+--------+-------+---+-------------------+------+------+-------------+-------------+------------+------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Stops with detected home/work location\n",
    "stops_c_hw2.orderBy(['useruuid', 'start']).withColumn(\"start_ts\", F.to_timestamp(\"start\"))\\\n",
    "    .withColumn(\"end_ts\", F.to_timestamp(\"end\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb0f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                       (0 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+---+-------------+------------+------------+\n",
      "|useruuid|s_yy|s_woy|loc|location_type|detect_H_loc|detect_W_loc|\n",
      "+--------+----+-----+---+-------------+------------+------------+\n",
      "|       1|1970|    2|  0|            O|           1|        null|\n",
      "|       1|1970|    2|  1|            H|           1|        null|\n",
      "|       1|1970|    2|  2|            O|           1|        null|\n",
      "|       1|1970|    3|  1|            H|           1|        null|\n",
      "+--------+----+-----+---+-------------+------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ---------------------- ALIGN STOPS DATASET TO LABELLED DATA TIME-RESOLUTION ----------------------\n",
    "# -- STEP 1: Assign week-year to detected labels -----------------------------\n",
    "detectlocs_wy = (\n",
    "    stops_c_hw2.withColumn(\n",
    "        \"date\",\n",
    "        F.date_format(F.from_unixtime(\"start\").cast(\"timestamp\"), \"yyyy-MM-dd\"),\n",
    "    )\n",
    "    .withColumn(\"s_woy\", F.weekofyear(\"date\"))  # Week of year\n",
    "    .withColumn(\"s_yy\", F.year(\"date\"))  # Year\n",
    "    .select([\"useruuid\", \"s_yy\", \"s_woy\", \"loc\", \"location_type\", \"detect_H_loc\", \"detect_W_loc\"])\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# - Show sample of week-level detected labels\n",
    "detectlocs_wy.orderBy([\"useruuid\", \"s_yy\", \"s_woy\", \"loc\"]).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e7aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GET HOME/WORK ACCURACY AND FRACTION OF NOT DETCED AT user-week level ----------------------\n",
    "\n",
    "def evaluate_weekly_detection_accuracy(detectlocs_wy, truelabels_wy, target_label: str)-> \"DataFrame\":\n",
    "    \"\"\"\n",
    "    Evaluate weekly accuracy of detected Home or Work locations. \n",
    "    A match is counted when the detected location matches\n",
    "    the annotated (true) location for the same user and week.\n",
    "\n",
    "    Parameters:\n",
    "    - detectlocs_wy: Spark DataFrame with columns ['useruuid', 's_woy', 's_yy', 'loc', 'location_type']\n",
    "                     The weekly-detected locations with estimated labels.\n",
    "    - truelabels_wy: Spark DataFrame with columns ['useruuid', 's_woy', 's_yy', 'loc', 'true_location_type']\n",
    "                     The weekly-labeled ground truth data.\n",
    "    - target_label: 'H' for Home or 'W' for Work\n",
    "\n",
    "    Returns:\n",
    "    - Spark DataFrame with columns:\n",
    "        count_:     total user-week-locations with known true label\n",
    "        match_sum:  total matches between detected and true label\n",
    "        detected_X:   number of weeks with detection of target label\n",
    "        acc_X:      accuracy among detected weeks\n",
    "        none_X:     percentage of weeks with no detection\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------- STEP 1: Flag if any target label type was detected per user-week ----------------------\n",
    "    window_uw = Window.partitionBy(\"useruuid\", \"s_woy\", \"s_yy\")\n",
    "\n",
    "    detect_with_flag = detectlocs_wy.withColumn(\n",
    "        f\"hasdetected_{target_label}_uw\",\n",
    "        F.max(F.when(F.col(f'detect_{target_label}_loc').isNotNull(), 1).otherwise(0)).over(window_uw)\n",
    "    )\n",
    "\n",
    "    # ---------------------- STEP 2: Filter annotated weekly labels of the given type and join detection info -------------------\n",
    "    truelabels_target_detectflag = truelabels_wy.filter(\n",
    "        F.col(\"true_location_type\") == target_label\n",
    "    ).join(\n",
    "        detect_with_flag.select(\"useruuid\", \"s_woy\", \"s_yy\", f\"hasdetected_{target_label}_uw\").dropDuplicates(),\n",
    "        on=[\"useruuid\", \"s_woy\", \"s_yy\"]\n",
    "    )\n",
    "\n",
    "    # ---------------------- STEP 3: Keep only one detected label for each user-week-loc  ----------------------\n",
    "    # -- Attention: In a given  week, a loc can have multiple labels detected,  we consider a match if one of this labels is the target label\n",
    "    w_uwloc = Window.partitionBy(\"useruuid\", \"s_woy\", \"s_yy\", \"loc\").orderBy(F.desc(\"is_target_label_uwloc\"))\n",
    "    \n",
    "    detect_with_flag_agg_wloc  = detect_with_flag.withColumn(\n",
    "        'is_target_label_uwloc', F.when(F.col('loc') == F.col(f'detect_{target_label}_loc'), F.lit(1)).otherwise(F.lit(0))\n",
    "    ).withColumn(\"sortby_bestlabel_uwloc\", F.row_number().over(w_uwloc)\n",
    "    ).filter(\n",
    "        F.col(\"sortby_bestlabel_uwloc\") == 1\n",
    "    ).drop(\"is_target_label_uwloc\", \"sortby_bestlabel_uwloc\")\n",
    "\n",
    "    # ---------------------- STEP 4: Match annotated location with detected location ----------------------\n",
    "    match_weekly = truelabels_target_detectflag.join(\n",
    "        detect_with_flag_agg_wloc.select(\"useruuid\", \"s_woy\", \"s_yy\", \"loc\", \"location_type\", f'detect_{target_label}_loc'),\n",
    "        on=[\"useruuid\", \"s_woy\", \"s_yy\", \"loc\"]\n",
    "    ).withColumn(\n",
    "        f\"match_{target_label}label_uw\",\n",
    "        F.when(F.col(\"loc\") == F.col(f'detect_{target_label}_loc'), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # ---------------------- STEP 5: Aggregate counts, compute summary stats ----------------------\n",
    "    agg = match_weekly.agg(\n",
    "        F.count(f\"match_{target_label}label_uw\").alias(f\"count_{target_label}\"),\n",
    "        F.sum(f\"match_{target_label}label_uw\").alias(f\"match_sum_{target_label}\"),\n",
    "        F.sum(f\"hasdetected_{target_label}_uw\").alias(f\"detected_{target_label}\")\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        agg.withColumn(\n",
    "            f\"acc_{target_label}\",\n",
    "            F.round(100.0 * F.col(f\"match_sum_{target_label}\") / F.col(f\"detected_{target_label}\"), 2)\n",
    "        ).withColumn(\n",
    "            f\"none_{target_label}\",\n",
    "\n",
    "            F.round(100.0 * (F.col(f\"count_{target_label}\") - F.col(f\"detected_{target_label}\")) / F.col(f\"count_{target_label}\"), 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb941f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-----+------+\n",
      "|count_H|match_sum_H|detected_H|acc_H|none_H|\n",
      "+-------+-----------+----------+-----+------+\n",
      "|   3196|       2593|      2734|94.84| 14.46|\n",
      "+-------+-----------+----------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-----+------+\n",
      "|count_W|match_sum_W|detected_W|acc_W|none_W|\n",
      "+-------+-----------+----------+-----+------+\n",
      "|   2953|       1875|      2265|82.78|  23.3|\n",
      "+-------+-----------+----------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Home\n",
    "res_weekly_H = evaluate_weekly_detection_accuracy(detectlocs_wy, truelabels_wy, target_label=\"H\")\n",
    "res_weekly_H.show()\n",
    "\n",
    "# Work\n",
    "res_weekly_W = evaluate_weekly_detection_accuracy(detectlocs_wy, truelabels_wy, target_label=\"W\")\n",
    "res_weekly_W.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94739a",
   "metadata": {},
   "source": [
    "## Validation against D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- IMPORT HOME/WORK GROUND TRUTH - USER LEVEL ----------------------\n",
    "fname = 'D2_truelabels_u'\n",
    "truelabels_u = spark.read.format(\"parquet\").load(PATH+fname, pathGlobFilter=\"*.parquet\")\n",
    "\n",
    "# - Show sample of week-level true labels\n",
    "truelabels_u.orderBy([\"useruuid\",\"loc\"]).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5257891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+------+-------+\n",
      "|useruuid|loc| start|   end|country|\n",
      "+--------+---+------+------+-------+\n",
      "|       1|  4|526200|528000|     ID|\n",
      "|       1|  5|546600|556200|     ID|\n",
      "|       1|  1|556800|562800|     ID|\n",
      "|       1|  1|589800|592800|     ID|\n",
      "|       1|  1|640200|643200|     ID|\n",
      "|       1|  1|648000|649800|     ID|\n",
      "|       1|  2|666000|666600|     ID|\n",
      "|       1|  3|667800|671400|     ID|\n",
      "|       1|  0|739800|741000|     ID|\n",
      "|       1|  1|756000|757200|     ID|\n",
      "|       1|  1|775200|775800|     ID|\n",
      "|       1|  6|779400|780000|     ID|\n",
      "|       1|  1|795000|801000|     ID|\n",
      "|       1|  7|836400|838800|     ID|\n",
      "|       1|  8|843000|844200|     ID|\n",
      "|       1|  1|861600|864000|     ID|\n",
      "|       1|  1|864000|865800|     ID|\n",
      "|       1|  9|901800|910800|     ID|\n",
      "|       1| 10|919800|925800|     ID|\n",
      "|       1| 10|930000|930600|     ID|\n",
      "+--------+---+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- IMPORT STOPS ----------------------\n",
    "fname = 'D2_stops'\n",
    "stops_anonym_sample = spark.read.format(\"parquet\").load(PATH+fname, pathGlobFilter=\"*.parquet\")\n",
    "stops_anonym_sample.orderBy(['useruuid', 'start']).show()\n",
    "print(stops_anonym_sample.select('useruuid').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "264bb0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/27 13:57:37 WARN CacheManager: Asked to cache already cached data.\n",
      "HoWDe Labelling: computing LABs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> stops pre-processed\n",
      " >>> home/works detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> output formatted as stops: True\n",
      "HoWDe Labelling: computations completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # ---------------------- HOME/WORK DETECTION with HoWDe ----------------------\n",
    "stops_c_hw2 = HoWDe_labelling(\n",
    "    input_data = stops_anonym_sample,\n",
    "    spark = spark,\n",
    "    range_window_home = 730, ### >> ATTENTION: Here we set-up 730days to reproduce the view of the annotaters, this is not the intended use of the window\n",
    "    range_window_work = 730, ### >> ATTENTION: Here we set-up 730days to reproduce the view of the annotaters, this is not the intended use of the window\n",
    "    dhn = 3,   \n",
    "    dn_H = 0.8, ### >> Increasing allowed fraction of missing days in window to compensate for the long windows\n",
    "    dn_W = 0.8, ### >> Increasing allowed fraction of missing days in window to compensate for the long windows\n",
    "    hf_H = 0.7, \n",
    "    hf_W = 0.4, \n",
    "    df_W = 0.8, \n",
    "    stops_output = True,\n",
    "    verbose = True,\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5166cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---+-------------------+------+------+-------------+-------------+------------+------------+-------------------+-------------------+\n",
      "|useruuid|country|loc|date               |start |end   |stop_duration|location_type|detect_H_loc|detect_W_loc|start_ts           |end_ts             |\n",
      "+--------+-------+---+-------------------+------+------+-------------+-------------+------------+------------+-------------------+-------------------+\n",
      "|1       |ID     |4  |1970-01-07 00:00:00|526200|528000|1800         |O            |null        |null        |1970-01-07 02:10:00|1970-01-07 02:40:00|\n",
      "|1       |ID     |5  |1970-01-07 00:00:00|546600|556200|9600         |O            |null        |null        |1970-01-07 07:50:00|1970-01-07 10:30:00|\n",
      "|1       |ID     |1  |1970-01-07 00:00:00|556800|562800|6000         |O            |null        |null        |1970-01-07 10:40:00|1970-01-07 12:20:00|\n",
      "|1       |ID     |1  |1970-01-07 00:00:00|589800|592800|3000         |O            |null        |null        |1970-01-07 19:50:00|1970-01-07 20:40:00|\n",
      "|1       |ID     |1  |1970-01-08 00:00:00|640200|643200|3000         |O            |null        |null        |1970-01-08 09:50:00|1970-01-08 10:40:00|\n",
      "|1       |ID     |1  |1970-01-08 00:00:00|648000|649800|1800         |O            |null        |null        |1970-01-08 12:00:00|1970-01-08 12:30:00|\n",
      "|1       |ID     |2  |1970-01-08 00:00:00|666000|666600|600          |O            |null        |null        |1970-01-08 17:00:00|1970-01-08 17:10:00|\n",
      "|1       |ID     |3  |1970-01-08 00:00:00|667800|671400|3600         |O            |null        |null        |1970-01-08 17:30:00|1970-01-08 18:30:00|\n",
      "|1       |ID     |0  |1970-01-09 00:00:00|739800|741000|1200         |O            |null        |null        |1970-01-09 13:30:00|1970-01-09 13:50:00|\n",
      "|1       |ID     |1  |1970-01-09 00:00:00|756000|757200|1200         |O            |null        |null        |1970-01-09 18:00:00|1970-01-09 18:20:00|\n",
      "+--------+-------+---+-------------------+------+------+-------------+-------------+------------+------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Stops with detected home/work location\n",
    "stops_c_hw2.orderBy(['useruuid', 'start']).withColumn(\"start_ts\", F.to_timestamp(\"start\"))\\\n",
    "    .withColumn(\"end_ts\", F.to_timestamp(\"end\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a470970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------+\n",
      "|useruuid|loc|location_type|\n",
      "+--------+---+-------------+\n",
      "|       2|  0|            H|\n",
      "|       2|  5|            W|\n",
      "|       3|  4|            H|\n",
      "|       3|  5|            W|\n",
      "|       4|  0|            H|\n",
      "|       4|  4|            W|\n",
      "|       5|  0|            H|\n",
      "|       6|  0|            H|\n",
      "|       6|  1|            W|\n",
      "|       7|  5|            H|\n",
      "|       8|  0|            H|\n",
      "|      10|  0|            H|\n",
      "|      10|  6|            W|\n",
      "|      11|  4|            W|\n",
      "|      11|  5|            H|\n",
      "|      12|  0|            H|\n",
      "|      13|  0|            H|\n",
      "|      14|  0|            H|\n",
      "|      14| 12|            W|\n",
      "|      15|  0|            H|\n",
      "+--------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- ALIGN STOPS DATASET TO LABELLED DATA TIME-RESOLUTION ----------------------\n",
    "# -- STEP 1: Prepare user-location detected labels -----------------------------\n",
    "detectlocs_u = stops_c_hw2.select([\"useruuid\", \"loc\", \"location_type\"]).dropDuplicates()\n",
    "detectlocs_u.filter(F.col('location_type')!='O').orderBy([\"useruuid\", \"loc\"]).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d005073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GET HOME/WORK ACCURACY AND FRACTION OF NOT DETCED AT user level ----------------------\n",
    "def evaluate_userlevel_accuracy(truelabels_u, detectlocs_u, target_label: str,  it_cols: list = []) -> \"DataFrame\":\n",
    "    \"\"\"\n",
    "    Evaluate user-level detection accuracy for Home or Work labels.\n",
    "    A match is counted if any detected location matches the annotated true location\n",
    "    for the same user at any time.\n",
    "\n",
    "    Parameters:\n",
    "    - truelabels_u: Spark DataFrame with columns ['useruuid', 'loc', 'true_location_type', ...]\n",
    "                    Annotated true labels at user-location level.\n",
    "    - detectlocs_u: Spark DataFrame with columns ['useruuid', 'loc', 'location_type', ...]\n",
    "                    Detected locations at user-level.\n",
    "    - target_label: 'H' for Home or 'W' for Work\n",
    "    - it_cols:      Optional list of grouping columns for stratified results (e.g., ['iteration'])\n",
    "\n",
    "    Returns:\n",
    "    - Spark DataFrame with:\n",
    "        count_X:     Number of annotated users\n",
    "        match_sum_X: Number of correctly detected rows (label matches)\n",
    "        detected_X:  Number of users with any detection of the target label\n",
    "        acc_X:       Accuracy of detected labels (rows matched / rows detected)\n",
    "        none_X:      % of annotated users with no detection\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) join & Boolean flags\n",
    "    joined = (\n",
    "        truelabels_u.alias(\"t\")\n",
    "        .join(detectlocs_u.alias(\"d\"), [\"useruuid\", \"loc\"], \"left\")\n",
    "        .select( *it_cols, \"useruuid\", \"loc\",\n",
    "                 (F.col(\"t.true_location_type\") == target_label).cast(\"int\").alias(\"true_f\"),\n",
    "                 (F.col(\"d.location_type\")      == target_label).cast(\"int\").alias(\"det_f\") )\n",
    "    )\n",
    "    # 2) one row per (iter, user, loc) keeping any detection of the label\n",
    "    uloc = (\n",
    "        joined.groupBy( *it_cols, \"useruuid\", \"loc\")\n",
    "              .agg( F.max(\"true_f\").alias(\"true_f\"),\n",
    "                    F.max(\"det_f\").alias(\"det_f\") )\n",
    "    )\n",
    "    # 3) user-level roll-up\n",
    "    per_user = (\n",
    "        uloc.groupBy( *it_cols, \"useruuid\")\n",
    "            .agg(\n",
    "                F.max(\"true_f\").alias(\"has_true\"),         # user is annotated for the label\n",
    "                F.max(\"det_f\").alias(\"has_detect\"),        # user ever detected the label\n",
    "                F.sum(\"true_f\").alias(\"annot_rows\"),\n",
    "                F.sum( F.when( (F.col(\"true_f\") == 1) & (F.col(\"det_f\") == 1), 1)\n",
    "                       .otherwise(0) ).alias(\"match_rows\") )\n",
    "            .filter(\"has_true = 1\")                        # only annotated users\n",
    "    )\n",
    "    # 4) final metrics\n",
    "    agg = (\n",
    "        per_user.groupBy( *it_cols )\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"count_u\"),\n",
    "                F.sum(\"has_detect\").alias(\"wdetec_u\"),\n",
    "                F.sum( F.when( F.col(\"has_detect\") == 1, F.col(\"annot_rows\") )\n",
    "                       .otherwise(0) ).alias(\"total_rows\"),\n",
    "                F.sum( F.when( F.col(\"has_detect\") == 1, F.col(\"match_rows\") )\n",
    "                       .otherwise(0) ).alias(\"match_sum\") )\n",
    "            .withColumn(\"acc\",   F.col(\"match_sum\") / F.col(\"total_rows\"))\n",
    "            .withColumn(\"none\",  1 - F.col(\"wdetec_u\") / F.col(\"count_u\"))\n",
    "    )\n",
    "\n",
    "    sel = it_cols + [\n",
    "        f\"count_u        as count_{target_label}\",\n",
    "        f\"match_sum      as match_sum_{target_label}\",\n",
    "        f\"wdetec_u       as detected_{target_label}\",\n",
    "        f\"ROUND(acc*100, 2)  as acc_{target_label}\",\n",
    "        f\"ROUND(none*100,2) as none_{target_label}\",\n",
    "    ]\n",
    "    return agg.selectExpr(*sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8591e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-----+------+\n",
      "|count_H|match_sum_H|detected_H|acc_H|none_H|\n",
      "+-------+-----------+----------+-----+------+\n",
      "|    500|        393|       395| 75.0|  21.0|\n",
      "+-------+-----------+----------+-----+------+\n",
      "\n",
      "+-------+-----------+----------+-----+------+\n",
      "|count_W|match_sum_W|detected_W|acc_W|none_W|\n",
      "+-------+-----------+----------+-----+------+\n",
      "|    287|        149|       168|73.76| 41.46|\n",
      "+-------+-----------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Home\n",
    "res_u_H = evaluate_userlevel_accuracy(truelabels_u, detectlocs_u, target_label=\"H\")\n",
    "res_u_H.show()\n",
    "\n",
    "# Work\n",
    "res_u_W = evaluate_userlevel_accuracy(truelabels_u, detectlocs_u, target_label=\"W\")\n",
    "res_u_W.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "howde_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
